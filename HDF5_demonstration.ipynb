{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 Tutorial and Demonstration\n",
    "\n",
    "***\n",
    "\n",
    "### Welcome! This notebook will show you how to use the functions associated with ACEP_Solar's HDF5 Data Storage, and give a brief overview of its intended usage. We at the ACEP_Solar Dev Team hope that you find it useful and clear. \n",
    "\n",
    "This data storage is done through the use of HDF5, or [Hierarchical Data Format 5,](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) which is a highly compressed data storage type. It stores data in binary format, so it is transferrable across operating systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our intention with the HDF5 system is for it to be used hierarchically, to help organize and sort data, though you may choose to use it any way that you wish. Just note that if a different format or architecture is chosen, certain components of the code may need to be reorganized. We think we've chosen a reasonable structure, and we hope that you'll find it to be to your liking though! \n",
    "\n",
    "The structure we have chosen is to store all data in a single HDF5 file, a choice largely made for ease of loading data into later components of the package. The first level of hierarchy within the HDF5 file, or the first group underneath the HDF5 file header, is names that correspond to the locations of various locales in Alaska (ie, Juneau, Fairbanks, Anchorage). This serves as our first delineator - all data is sorted prior to insertion into the HDF5 file. Note that all things will be grouped based on the location name, so if one location is named Juneau, and the other is named Juneau, AK, they will not be stored in the same location! \n",
    "\n",
    "Within each location group, there exists a group that stores all the data for a particular solar installation - These can be named whatever you would like. Inside the installation group, stored as individual datasets are the various parameters of that installation, including things like energy produced. Stored within that installation group is also an attribute that lists the system's DC capacity, to allow for energy production normalization.  \n",
    "\n",
    "With that description done, lets walk through how this plays out in our functions. One thing to note is that most of the interactions herein do not result in a readily interactable result, so we'll be somewhat limited in our examples here. Other functionality (LINK) will make use of reading this data, and detailed examples of reading it from an HDF5 file will be provided therein.\n",
    "\n",
    "**NOTE!** All data is expected to be sorted into a specific format prior to storage in the HDF5 file, otherwise all of these functions will fail. Please see the example.xslx file for an example of the expected format, found here at our Github page (insert link!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Creating the HDF5 File\n",
    "\n",
    "The first function is to create the HDF5 file on your disk. If you already have an HDF5 file created that you would like to use for the data storage file, you can skip this step.\n",
    "\n",
    "The function is `create_hdf5_file`. It takes one argument, 'hdf5_filename', which is the name that you would like the file to be saved under. The function will throw an error if you attempt to create a filename that already exists. We'll call that now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_interface.create_hdf5_file('demo_filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now we will have a hdf5 filetype in our directory with 'demo_filename.hdf5' as the filename. Now, we'll begin to create the internal structure of our hdf5 files with our other functions.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating groups and datasets within the HDF5 File \n",
    "\n",
    "The second function, `add_to_hdf5_file`, allows the entry of individual solar installation data. It takes three inputs, the filename of the hdf5 file, the name of the solar installation, and the name of the file that the solar installation data is saved within. The datafile can be either a csv or an xslx file. Please refer to the example.xslx and example.csv files in the github repo (LINK) for examples of how to format these files. *Note!* The data_filename should include the file extension! if the function isn't passed a file that ends with .csv or .xslx it will throw an error.\n",
    "\n",
    "This function searches the data file and pulls a location from that data file. It searches within the HDF5 filestructure to find the location, and if it is not found, it adds the location into the HDF5 filestrucutre. After finding or adding the location, the function reads the data from that data file, and saves the data into the HDF5 file under three separate datasets: Energy, Month, and Year, or under four separate datasets, Energy, Day, Month, and Year, depending on the resolution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdf5_interface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-94544d2f5e61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhdf5_interface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_hdf5_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hdf5_filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data_filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'solar_installation_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'hdf5_interface' is not defined"
     ]
    }
   ],
   "source": [
    "hdf5_interface.add_to_hdf5_file('hdf5_filename', 'data_filename', 'solar_installation_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! That shows the command for how the software reads and adds in a brand new solar installation entry. Note that this only works for *NEW* entries. If you want to update an existing entry, say with new data, we will need to use a different functionality. Though this seems odd, it's part of the functionality of HDF5. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating existing entries in the HDF5 system\n",
    "\n",
    "The third function, `update_existing_panel_entry`, enables the updating of an existing panel entry. Like with the addition of a new entry, this function takes three inputs: the filename of the HDF5 file, the name of the solar installation, and the name of the file that the solar installation data is saved within. The datafile again can be either a csv or an xslx file. Please refer to the example.xslx and example.csv files in the github repo (LINK) for examples of how to format these files. *Note!* The data_filename should include the file extension! if the function isn't passed a file that ends with .csv or .xslx it will throw an error.\n",
    "\n",
    "This function finds the currently existing entry in the HDF5 file, and deletes the existing entry's datasets (Energy, Month, and Year, or Energy, Day, Month, and Year), and then adds in the new datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_interface.update_existing_panel_entry('hdf5_filename', 'data_filename', 'solar_installation_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that function call, we have updated the \"solar_installation_name\" installation data within our \"hdf5_filename\" file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "In addition to these upfront functions, there is a 'hidden' helper function, called `extract_file_to_dataframe`, which pulls down the excel spreadsheet or csv filename, and opens it from your hard drive into a pandas dataframe. This dataframe is the structure by which all the other functions interact with your data, so it's an important step, but isn't something that is likely important to interact with; however, if in your program you find that some modification to how the code is read in is needed, some modifications here could be important. \n",
    "\n",
    "Thanks again for working with our code. \n",
    "-ACEP solar dev team. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
